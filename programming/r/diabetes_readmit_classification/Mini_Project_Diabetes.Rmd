---
title: "Predicting diabetes related hospital readmission"
author: "Joseph Haymaker"
date: 'Due: November 19, 2017 at 11:59PM'
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    code_folding: show
    highlight: haddock
    number_sections: yes
    self_contained: yes
    theme: lumen
    toc: yes
    toc_depth: 4
    toc_float: yes
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyfoot[CO,CE]{}
- \fancyfoot[LE,RO]{\thepage}
subtitle: STAT 471/571/701, Fall 2017
graphics: yes
---

```{r, echo=FALSE, fig.align="center", out.width="300px"}
knitr::include_graphics("/Users/josephhaymaker/Desktop/STAT571_Modern-data-mining/Mini Project_DiabeticStudy/diabetes1.png")
```

```{r setup, include=FALSE}
# knitr::opts_chunk$set(tidy=TRUE, fig.width=6,  fig.height=5, 
#                       fig.align='left', dev = 'pdf')
knitr::opts_chunk$set(fig.height=5, fig.width=11, warning = F, fig.align = "left")
# install.packages('plotrix')
library(car)       # v 2.1-4
library(glmnet)    # v 2.0-5
library(dplyr)     # v 0.5.0
library(ggplot2)
library(GGally)
library(plotrix)
library(randomForest)
library(tree)
library(rpart)
library(rpart.plot)
library(pROC)
library(partykit)
library(gbm)
```

# Executive Summary

For most of 2017 health care has been in the forefront of public discourse due to president Trump's campaign promise to "repeal and replace Obamacare" (the Affordable Care Act). The majority of the deleterious macro effects these changes would have on the American public have highlighted losses in coverage and the disproportionate effect to lower income and elderly individuals ([source 1](http://money.cnn.com/2017/07/14/news/economy/obamacare-repeal-senate-health-care-bill/index.html), [source 2](https://www.nytimes.com/2017/06/22/us/politics/senate-health-care-bill.html)). To a lesser extent, these appraisals have addressed changes affecting Americans with pre-existing conditions and funding cuts hospitals would experience ([source](https://www.npr.org/sections/health-shots/2017/07/10/535851043/faq-how-would-the-republican-health-care-bills-affect-you)). Even in spite of this ongoing national debate around the topic, healthcare costs and savings in the US are a perennially obtuse topic, even to most Americans. 

This investigation seeks to focus on one particular aspect of healthcare costs and savings in the US: hospitalization costs, specifically the cost of readmitting a diabetes patient in less than 30 days. This inquiry intends to address this issue by providing healthcare providers information about a patient's estimated propensity for readmission in this time period given certain individual factors. It is our hope that the findings here shed light on patient characterisitics influencing readmission, which the care community may then turn into actionable policy and practice changes that both better serve patients as well as reduce readmission costs.   

The dataset analyzed is courtesy of Virginia Commonwealth University and consists of patient observations from 130 hospitals in the US from 1999 - 2008. The original dataset contained 50 patient features across 101,766 observations. In the simplest terms the variables belong to the following categories: patient identifiers, patient demographics, admission and discharge details, patient medical history, patient admission details, clinical results, medication details, & readmission indicator (dependent variable). After cleaning many of the medication details were eliminated due to little variability, while binning was applyed to some categorical variables. Similarly, througout the process variables with large amounts of missing entries, those with an excessive amount of levels, and ones unrelated to readmission status were eliminated. 

Three classification models were obtained from this data: two using logistic regression and one using the random forests technique. The implications of these models are that to prevent readmission and better serve patients the hospital should pay particular attention to the following patient characteristics and their purported effects on readmission under 30 days: 

+ admission source/patient referral
+ patient age
+ diabetes medication prescribed during visit (y/n) 
+ discharge disposition (discharged to home, discharged to home w/ home health services, discharged/transferred to SNF (skilled nursing facilities), & other)
+ insulin dose change during visit (down, no, steady, or up)
+ Metformin medication change (down, no, steady, or up),total number of diagnosis for patient 
+ Number of emergency visits by the patient in the year prior to the current encounter
+ Number of inpatient visits by the patient in the year prior to the current encounter
+ Patient’s length of stay in the hospital (in days)

With respect to factors that increase likelihood of readmission <30 days, unsurprisingly, probability of short term readmission increase as number of emergency visits, number of inpatient visits, time in hospital, and number of diagnoses per patient increase. Other more abstract factors increasing the probability are physician referral for admission, diabetes medication prescription during visit, and virtually any discharge situation, though discharge to 'other' situation and skilled nursing facilities cause a greater increase in this probability. There may be something of a selection effect in that finding, however. Lastly, all ages over 19 imply an increase in the probability, though the 60-79 age range corresponds with the greatest increase in this probability.

Factors that decrease likelihood of readmission <30 days are the following: no insulin dosage prescribed, steady dosage, or increased dosage _all_ have a decreased effect on likelihood of short term readmission. Compared to an emergency room patient admission, both 'other' admits and referrals from home health imply a drop in likelihood of a <30 day readmission. 

By paying particular attention to these indicators hospitals may be able to identify patients of greater risk, and change their treatment plans accordingly, as well as connect them with more extensive social services when possible to avoid the current costs being incurred by these types of patients and their short term hospital readmits. 

Lastly, their are several problems, solutions, and ideas for further investigation that emerge from this dataset and these findings. The most salient question from the dataset is where the 130 hospitals are located. This is important due to the differing health trends of US regions. Thus, a large representation from unhealthier regions or states would imply the sample wasn't random. To speak to the different environmental factors of the hospitals future investigation might take environmental factors into account, such as weather trends (number of sunny days per year, for example), walkability scores of the areas, percent car owners in the areas, average commute time, proximity to grocery stores/ fresh food availabilty (or food deserts), etc. These would all serve as indicators of environmental factors that either cause or reflect a more sedentary lifestyle or poor eating habits, which in turn affect health outcomes. Another aspect of future investigation might be the role of early intervention such as health eating programs and exercise regimes à la Michelle Obama's "Let's Move!" campaign. Moreover, with the data as is there remain many questions about correlation of variables. For example, a better way to ascertain if it was diabetes that caused hospitalization, or a concurrent/opportunistic complication. Lastly, one important thing to remember about this data is that it only covers outcomes up through 2008. It was only in 2014 that the major provisions of the Affordable Care Act came into effect. Up until that point many of these people would likely have been shut out of certain types of coverage due to pre-existing condition status. Since then they may have experienced greater access to preventative care, which in turn would reflect in the readmission rate. A more current dataset would be needed to assess this.

# Detailed Process of the Analysis

### Introduction

In the _National Diabetes Statistics Report, 2017_ the CDC estimates that 30.3 million people of all ages—or 9.4% of the U.S. population—had diabetes as of 2015 ([source](https://www.cdc.gov/diabetes/pdfs/data/statistics/national-diabetes-statistics-report.pdf)). Moreover, the CDC estimates both direct and indirect costs due to diabetes in the United States in 2012 was \$245 billion. Average medical expenditures for people with diagnosed diabetes were about \$13,700 per year, 60% of which was attributed to the illness directly. Medical expenditures among people with diagnosed diabetes were about 2.3 times higher than expenditures for people without diabetes ([source](https://www.cdc.gov/diabetes/pdfs/data/statistics/national-diabetes-statistics-report.pdf)). Consequently, the Centers for Medicare and Medicaid Services announced in 2012 that they would no longer reimburse hospitals for services rendered if a patient was readmitted with complications within 30 days of discharge. 

On the healthcare provider side the estimated cost burden resulting from avoidable hospitalizations due to short-term uncontrolled diabetes is equally staggering: \$2.8 billion ([source](http://care.diabetesjournals.org/content/30/5/1281)). Thus it is clear that hyperglycemia management and treatment is not only a hospital expenditure question, but a public health concern. This study seeks to use large amounts of patient data to develop models that can predict a patient's likelihood of readmission in less that 30 days, and thus provide healthcare providers with more tools to anticipate and avoid these outcomes.

### Data Summary

The original data is from the Center for Clinical and Translational Research at Virginia Commonwealth University. It covers data on diabetes patients across 130 U.S. hospitals from 1999 to 2008. There are 101,766 hospital admissions observations in this dataset, with 50 variables describing patients. The cleaned dataset contains only 31 variables. Pairing down was done due to large amounts of missing values (`Payer code`, `weight` and `Medical Specialty`, & entries with a '?' `race` value). Many of the medication variables had little variation and thus were eliminated. Binning was performed on the diagnoses variables. Lastly, `readmitted` was changed to be a categorical variable. The remaining variables still account for many patient characteristics such as demographics, medications, test results, and partial medical history (number of hospital visits, etc.)

<!--- ###################################### Dataset characteristics #################################--->
```{r, eval=FALSE, echo=FALSE}
## =============================  STANDARD EDA TECHNIQUES ==============================
## <<<< READING IN DATA >>>
## ===== FULL DATASET =====
df.full <- read.csv("diabetic.data.csv", header=TRUE, sep=",", na.strings="") # accounts for header, CSV, and na strings
dim(df.full) # 101,766 observations x 50 variables
head(df.full, 30)
# View(df.full)
# summary(df.full)
summary(df.full$readmitted)
```

```{r, include=FALSE}
# ====== CLEANED DATASET ====
data1 <- read.csv("readmission.csv", header=TRUE, sep=",", na.strings="") # accounts for header, CSV, and na strings
# dim(data1) #101766 observations x 31 variables
#tail(data1, 20)
# head(data1, 20)
# View(data1)
# levels(data1$diag1_mod)

# data1 <- data1[ -c(6, 11:12, 28, 30, 33, 36:41, 43:47) ]# getting rid of unhelpful vars
#names(data1)
#dim(data1) # 101766 x 33
# summary(data1)
#levels(data1$adm_src_mod)

# <<<<<<<<<< NA VALUES >>>>>>>>>
sum(is.na(data1))
# show how many NA values in each column
sapply(data1, function(x) sum(is.na(x))) # no 0 values
```

__Data source:__
Clore, John, et al. “ Diabetes 130-US Hospitals for Years 1999-2008 Data Set .” UCI Machine Learning Repository: Diabetes 130-US Hospitals for Years 1999-2008 Data Set, Center for Clinical and Translational Research, Virginia Commonwealth University, 2014, [archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008](archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008).

### Modeling & Analyses

#### Variables of interest

For the sake of getting a better idea about the data and trends, I isolated several variables that seem likely to influence readmission. These variables are `readmitted`, `race`, `gender`, and `number_diagnoses` (note: I originally also had `age`, `change`, but they have been moved to the [appendix](#further-eda) for the sake of brevity). I have summarized and visualized my findings below. 

[See appendix for more detailed information on variables](#variable-names-and-detail-explanations)

##### Readmitted
```{r, eval=FALSE, include=FALSE}
summary(data1$readmitted)
#  <30   >30    NO 
# 11357 35545 54864 
perc.less30 <- 11357/(11357+35545+54864)
perc.less30 #11%
perc.more30 <- 35545/(11357+35545+54864)
perc.more30 #35%
```
Before changing this dependent variable to be categorical (readmitted <30 days = 1, not readmitted <30 days = 0) it's worth noting that the patients readmitted <30 days comprise ~11% of all patients, while those readmitted over 30 days is 35%, and those not readmitted is the remaining 54%. 

##### Race

Since many health issues affect certain racial groups disproportionately, it seems reasonable to explore the relationship between race and readmission. Causasians and african americans observations dwarf the remaining 3 categories; there are (roughly) 76,000 Caucasian patients and 20,000 African American patients as opposed to only 650 Asian, 2050 Hispanic, and 1500 'other' patients. Visualizing the breakdown of readmit status for these groups provides us with some interesting information:

```{r, echo=FALSE}
#variables of interest
# summary(data1$race) # boxplot readmit by race

# filter by race (AfricanAmerican, Asian, Caucasian, Hispanic, Other) &&  
# ------ AfricanAmerican ----
readmit_less30.afamer <- filter(data1, race == "AfricanAmerican", readmitted == "<30")
# dim(readmit_less30.afamer) # 2155
readmit_more30.afamer <- filter(data1, race == "AfricanAmerican", readmitted == ">30")
# dim(readmit_more30.afamer) # 6634
readmit_none.afamer <- filter(data1, race == "AfricanAmerican", readmitted == "NO")
# dim(readmit_none.afamer) # 10421
slices.afamer <- c(2155, 6634, 10421) 
lbls.afamer <- c("<30", ">30", "none")
pct.afamer <- round(slices.afamer/sum(slices.afamer)*100)
lbls.afamer <- paste(lbls.afamer, "-(",pct.afamer, ")") # add percents to labels 
lbls.afamer <- paste(lbls.afamer,"%",sep="") # ad % to labels 

pie(slices.afamer,labels = lbls.afamer, col=rainbow(length(lbls.afamer)),
  	main="Pie Chart of African American Readmits")
```

```{r, echo=FALSE}
# ---- ASIAN ----
readmit_less30.asian <- filter(data1, race == "Asian", readmitted == "<30")
# dim(readmit_less30.asian) # 65
readmit_more30.asian <- filter(data1, race == "Asian", readmitted == ">30")
# dim(readmit_more30.asian) # 161
readmit_none.asian <- filter(data1, race == "Asian", readmitted == "NO")
# dim(readmit_none.asian) # 415
slices.asian <- c(65, 161, 415) 
lbls.asian <- c("<30", ">30", "none")
pct.asian <- round(slices.asian/sum(slices.asian)*100)
lbls.asian <- paste(lbls.asian, "-(",pct.asian, ")") # add percents to labels 
lbls.asian <- paste(lbls.asian,"%",sep="") # ad % to labels 

pie(slices.asian,labels = lbls.asian, col=rainbow(length(lbls.asian)),
  	main="Pie Chart of Asian Readmits")
```

```{r, echo=FALSE}
# ---- CAUCASIAN ----
readmit_less30.cau <- filter(data1, race == "Caucasian", readmitted == "<30")
# dim(readmit_less30.cau) # 8592
readmit_more30.cau <- filter(data1, race == "Caucasian", readmitted == ">30")
# dim(readmit_more30.cau) # 27124
readmit_none.cau <- filter(data1, race == "Caucasian", readmitted == "NO")
# dim(readmit_none.cau) # 40383
slices.cau <- c(8592, 27124, 40383) #76099 total
lbls.cau <- c("<30", ">30", "none")
pct.cau <- round(slices.cau/sum(slices.cau)*100)
lbls.cau <- paste(lbls.cau, "-(",pct.cau, ")") # add percents to labels 
lbls.cau <- paste(lbls.cau,"%",sep="") # ad % to labels 

pie(slices.cau,labels = lbls.cau, col=rainbow(length(lbls.cau)),
   	main="Pie Chart of Caucasian Readmits")
```

```{r, echo=FALSE}
# ---- HISPANIC ----
readmit_less30.hisp <- filter(data1, race == "Hispanic", readmitted == "<30")
# dim(readmit_less30.hisp) # 212
readmit_more30.hisp <- filter(data1, race == "Hispanic", readmitted == ">30")
# dim(readmit_more30.hisp) # 27124
readmit_none.hisp <- filter(data1, race == "Hispanic", readmitted == "NO")
# dim(readmit_none.hisp) # 40383
slices.hisp <- c(212, 642, 1183) #76099 total
lbls.hisp <- c("<30", ">30", "none")
pct.hisp <- round(slices.hisp/sum(slices.hisp)*100)
lbls.hisp <- paste(lbls.hisp, "-(",pct.hisp, ")") # add percents to labels 
lbls.hisp <- paste(lbls.hisp,"%",sep="") # ad % to labels 

pie(slices.hisp,labels = lbls.hisp, col=rainbow(length(lbls.hisp)),
   	main="Pie Chart of Hispanic Readmits")
```

```{r, echo=FALSE}
# ---- OTHER ----
readmit_less30.oth <- filter(data1, race == "Other", readmitted == "<30")
# dim(readmit_less30.oth) # 145
readmit_more30.oth <- filter(data1, race == "Other", readmitted == ">30")
# dim(readmit_more30.oth) # 446
readmit_none.oth <- filter(data1, race == "Other", readmitted == "NO")
# dim(readmit_none.oth) # 915
slices.oth <- c(145, 446, 915)
lbls.oth <- c("<30", ">30", "none")
pct.oth <- round(slices.oth/sum(slices.oth)*100)
lbls.oth <- paste(lbls.oth, "-(",pct.oth, ")") # add percents to labels 
lbls.oth <- paste(lbls.oth,"%",sep="") # ad % to labels 

# ---Pie charts of RACE----
# par(mfrow = c(3, 2))
# pie(slices.afamer,labels = lbls.afamer, col=rainbow(length(lbls.afamer)),
#   	main="Pie Chart of African American Readmits")
# pie(slices.asian,labels = lbls.asian, col=rainbow(length(lbls.asian)),
#   	main="Pie Chart of Asian Readmits")
# pie(slices.cau,labels = lbls.cau, col=rainbow(length(lbls.cau)),
#    	main="Pie Chart of Caucasian Readmits")
# pie(slices.hisp,labels = lbls.hisp, col=rainbow(length(lbls.hisp)),
#    	main="Pie Chart of Hispanic Readmits")
pie(slices.oth,labels = lbls.oth, col=rainbow(length(lbls.hisp)),
   	main="Pie Chart of Other Races Readmits")

```
Regardless of race, roughly 10-11% of the patients are readmitted in less than 30 days. This implies race may not play such a big role in this outcome after all. All proportions are very close to one another across races, save for Asians which have the highest percentage of no readmits at 65%.

##### Gender

```{r, echo=FALSE}
# summary(data1$gender) #boxplot
         # Female            Male Unknown/Invalid 
         #  54708           47055               3 
readmit_less30.gender <- filter(data1, readmitted == "<30")
# dim(readmit_less30.gender) # 11357 total observations
# dim(filter(readmit_less30.gender, gender == "Female")) #6152 female ~54% of <30 dataset, 11.2% of females of total dataset
# dim(filter(readmit_less30.gender, gender == "Male")) #5205 male, 45% of <30 dataset, 11.1% of males of total dataset
readmit_more30.gender <- filter(data1, readmitted == ">30")

# nrow(readmit_more30.gender) #35545 total observations
# nrow(filter(readmit_more30.gender, gender == "Female")) #19518 female ~54% of >30 dataset, 35.7% of females of total dataset
# nrow(filter(readmit_more30.gender, gender == "Male")) #16027 male, 45%, 34.1% of males of total dataset
perc.female <- (19518/35545)
# perc.female #0.5491068
perc.male <- (16027/35545)
# perc.male # 0.4508932

par(mfrow = c(2, 1))
# nrow(which(readmit_less30.gender == "Female"))
# nrow(filter(readmit_less30.gender, gender == "Female"))
# nrow(readmit_less30.gender)
# 
# x.perc.gender <- c(nrow(filter(readmit_less30.gender, gender == "Female"))/nrow(readmit_less30.gender), nrow(filter(readmit_less30.gender, gender == "Male"))/nrow(readmit_less30.gender))
# x.perc.gender
ggplot(readmit_less30.gender) + geom_bar(aes(x = gender), fill = "blue") +
  labs(title = "Histogram of readmits in less than 30 days (<30) by gender", x = "Gender", y = "Frequency")
ggplot(readmit_more30.gender) + geom_bar(aes(x = gender), fill = "blue") +
  labs(title = "Histogram of readmits in more than 30 days (>30) by gender", x = "Gender", y = "Frequency")
```

In the cleaned dataset we have 54708 female observations and 47055 male observations, which means roughly 54% of the patients under consideration were female (for all readmission categories), while ~46% were male. When comparing hospital readmits striated by gender, of the patients that were readmitted in _under_ 30 days approximately 54% (6152/11357) were female, matching the overall female representation. Similarly, of patients that were readmitted _over_ 30 days again 54% (19518/35545) were female. Note that the total number of patients (male & female) readmitted over 30 days is about 3 times that of those readmitted in _less_ than 30 days. 

There seems to be a gap between genders here implying that women are more prone to readmission, but this is quickly rebuked when we compare the genders in terms of their total observations. For patients who were readmitted in _less_ than 30 days, female patients represent 11.2% (6152/54708) of the total female population, while those who are male represent a similar 11.1% (5205/47055) of the overall male population. The same is true for patients readmitted _over_ 30 days: female patients account for 35.7% (19518/54708) of the total female population, while male patients comprise 34.1% (16027/47055) of the total male population. 

This lends credence to the notion that gender does not contribute to likelihood of readmission. 

##### Number of diagnosis

```{r, echo=FALSE}
par(mfrow = c(2, 1))
# summary(data1$number_diagnoses) #bar plot
readmit_less30.diag <- filter(data1, readmitted == "<30")
# hist(readmit_less30.diag$number_diagnoses)
hist(readmit_less30.diag$number_diagnoses, 
     main="Histogram for number of diagnoses of patients readmitted <30 days", 
     xlab="Number of patient diagnoses", 
     border="blue", 
     col="green",
     xlim=c(0,16),
     las=1, 
     breaks=16)

readmit_more30.diag <- filter(data1, readmitted == ">30")
# hist(readmit_more30.diag$number_diagnoses)
hist(readmit_more30.diag$number_diagnoses, 
     main="Histogram for number of diagnoses of patients readmitted >30 days", 
     xlab="Number of patient diagnoses", 
     border="blue", 
     col="red",
     xlim=c(0,16),
     las=1, 
     breaks=16)
```

There consistently seems to be a large spike in frequency around 9 diagnoses, which greatly outnumbers the count of other diagnosis. 

<!---
##### Age vs. num medications
```{r}
summary(data1$age_mod) #scatterplot
summary(data1$num_medications)
#<<< SCATTERPLOT WITH LS LINE ADDED >>>>>
help(plot)
plot(data1$num_medications, data1$age_mod,
     pch = 20,
     col=data1$readmitted,
     xlab = "Patient number medications",
     ylab = "patient age",
     main = "Patient medication number vs. age")
```
--->

<!--- ================================================================================================================================ --->
<!--- ========================================== MODEL BUILDING ====================================================================== --->
<!--- ================================================================================================================================ --->

# Model building

#### Further prepping data

After investigating collinearity of variables (see [appendix](#investigating-collinearity)) I further modified the dataset by making `readmitted` categorical (readmitted <30 days = 1, otherwise = 0), removed all `race` entries coded as "?", and removed the `encounter_id` and `patient_nbr` columns as they add nothing to the model.

```{r, include=FALSE}
# <<<< make readmitted categorical>>>
# ======= readmitted = column 33 =======
# summary(data1)
# names(data1)
# summary(data1$readmitted) # needs to be changed to categorical
data1$readmitted <- factor(ifelse(data1$readmitted == "<30", "1", "0")) # if it's a less than 30 day readmit make it 1, else make it zero
# summary(data1$readmitted) # agrees with prior numbers
#     0     1 
# 90409 11357 

#<<<<<< remove all ? entries from race >>>>>>
#summary(data1$race) #2273 ? values -- remove them
#nrow(data1) #101766
data1 <- filter(data1, race != "?")
#nrow(data1) #99493  --> 99493+2273 = 101766
#summary(data1$race)

# <<<<<<< remove patient identifiers (not helpful) >>>>>
# encounter_id & patient_nbr
# summary(data1)
data1 <- subset(data1, select = -c(encounter_id,patient_nbr))
names(data1)
# df <- subset(df, select = -c(a,c) )

# filtering/ categorical data w/ if/else statement
#readmit_less30.diag <- filter(data1, readmitted == "<30")
#bill.data.train$status <- factor(ifelse(bill.data.train$status=="bill:passed" | bill.data.train$status=="governor:signed" | bill.data.train$status=="governor:received", "1", "0"))
summary(data1)  
table(data1$age_mod)
# levels(data1$diag_1) #717
# levels(data1$diag_2) #749
# levels(data1$diag_3) #790 
```

### Initial modeling

<!---
```{r, echo=FALSE}
# an initial linear model using probably variables
# summary(data1$max_glu_serum)
# summary(data1$insulin)
summary(data1$diabetesMed)
lm.init <- glm(readmitted~ race + gender + age_mod + time_in_hospital + num_medications + number_diagnoses + max_glu_serum + insulin + change + diabetesMed, data = data1, family = "binomial")
summary(lm.init)
Anova(lm.init)
```

An initial logistic model including logical variables reveals `race` and `gender` to add nothing to the model, while other factors (time in hospital, number of medications taken, number of diagnoses, max glucose serum, insulin, change in medication and diabetes medication) are all significant at the .05 level.
--->

```{r, include=FALSE}
#<<<<<<<<<<< LASSO >>>>>>>>>>>>>>
#<<<<<<CV to select lambda>>>>>>>
str(data1)
levels(data1$race)
set.seed(34)
names(data1)
# extract y, readmitted
Y <- data1$readmitted
#Y
X <- model.matrix(readmitted~. -diag1_mod -diag2_mod -diag3_mod, data = data1)[, -1]
#X
colnames(X)

# fit.lasso.cv <- cv.glmnet(X, Y, alpha = 1, nfolds = 10, family = "binomial")
# save(fit.lasso.cv, file="/Users/josephhaymaker/Desktop/STAT571_Modern-data-mining/HW4/model_lasso.RData")
load("/Users/josephhaymaker/Desktop/STAT571_Modern-data-mining/HW4/model_lasso.RData")

fit.lasso.cv$lambda.1se #0.006749408
fit.lasso.cv$lambda.min # 0.0003773466
# fit.lasso.cv$nzero

# plot(fit.lasso.cv$lambda , main  = "There are 100 lambdas used" , xlab = "Lambda Index" , ylab = "Lambda Value" ) 
#head(data.frame( Cross.Validation.Erorr = fit.lasso.cv$cvm , Lambda = fit.lasso.cv$lambda))            
plot(fit.lasso.cv$lambda, fit.lasso.cv$cvm, xlab=expression(lambda), ylab="mean cv errors")
plot(fit.lasso.cv)

# using λ=lambda.min
coef.min <- coef(fit.lasso.cv, s="lambda.min")  #s=c("lambda.1se","lambda.min") or lambda value
coef.min <- coef.min[which(coef.min !=0),]   # get the non=zero coefficients
coef.min  # the set of predictors chosen
rownames(as.matrix(coef.min)) # shows only names, not estimates -- 39 variables

#using λ=lambda.1se
 # coef.1se <- coef(fit.lasso.cv, s="lambda.1se")
 # coef.1se <- coef.1se[which(coef.1se !=0),]
 # coef.1se
 # rownames(as.matrix(coef.1se)) # only 4 variables -- too sparse -- ACTUALLY 8 with the modified dataset (actually reasonable)

# using all non-zero coefficients
# coef.nzero <-coef(fit.lasso.cv, nzero = 3)
# coef.nzero <- coef.nzero[which(coef.nzero !=0), ]
# rownames(as.matrix(coef.nzero))

# final-- using lambda.min
coef.min <- coef(fit.lasso.cv, s="lambda.min")  #s=c("lambda.1se","lambda.min") or lambda value
coef.min <- coef.min[which(coef.min !=0),]   # get the non=zero coefficients
coef.min 
rownames(as.matrix(coef.min)) 
#using LASSO variables to fit an lm() model
var.min <- rownames(as.matrix(coef.min)) 
var.min

#names(data1)
# "raceOther"                "genderMale"               "age[10-20)"               "age[40-50)"               "age[50-60)"              
#  [7] "age[70-80)"               "age[80-90)"               "age[90-100)"              "admission_type_id"        "discharge_disposition_id" "admission_source_id"     
# [13] "time_in_hospital"         "num_lab_procedures"       "num_procedures"           "num_medications"          "number_emergency"         "number_inpatient"        
# [19] "number_diagnoses"         "max_glu_serumNone"        "A1CresultNone"            "A1CresultNorm"            "metforminSteady"          "metforminUp"             
# [25] "repaglinideSteady"        "repaglinideUp"            "nateglinideSteady"        "nateglinideUp"            "glimepirideSteady"        "glipizideNo"             
# [31] "glipizideUp"              "glyburideNo"              "pioglitazoneSteady"       "rosiglitazoneNo"          "insulinNo"                "insulinSteady"           
# [37] "insulinUp"                "diabetesMedYes"          

# REVISION
#  [1] "(Intercept)"                                                    "raceOther"                                                     
#  [3] "genderMale"                                                     "time_in_hospital"                                              
#  [5] "num_lab_procedures"                                             "num_procedures"                                                
#  [7] "num_medications"                                                "number_outpatient"                                             
#  [9] "number_emergency"                                               "number_inpatient"                                              
# [11] "number_diagnoses"                                               "max_glu_serumNone"                                             
# [13] "A1CresultNone"                                                  "A1CresultNorm"                                                 
# [15] "metforminSteady"                                                "metforminUp"                                                   
# [17] "glimepirideSteady"                                              "glipizideNo"                                                   
# [19] "glipizideUp"                                                    "glyburideNo"                                                   
# [21] "pioglitazoneSteady"                                             "pioglitazoneUp"                                                
# [23] "rosiglitazoneNo"                                                "rosiglitazoneUp"                                               
# [25] "insulinNo"                                                      "insulinSteady"                                                 
# [27] "insulinUp"                                                      "diabetesMedYes"                                                
# [29] "disch_disp_modifiedDischarged to home with Home Health Service" "disch_disp_modifiedDischarged/Transferred to SNF"              
# [31] "disch_disp_modifiedOther"                                       "adm_src_modOther"                                              
# [33] "adm_src_modPhysician Referral"                                  "adm_src_modTransfer from Home Health"                          
# [35] "adm_typ_modEmergency"                                           "adm_typ_modOther"                                              
# [37] "adm_typ_modUrgent"                                              "age_mod60-79"                                                  
# [39] "age_mod80+" 

# lm.input <- as.formula(paste("readmitted", "~", paste(var.min[-1], collapse = "+")))
# lm.input <- "readmitted~ race + gender + age + time_in_hospital + num_lab_procedures + num_procedures + num_medications + number_emergency + number_inpatient + number_diagnoses + max_glu_serum + A1Cresult + metformin + repaglinide + nateglinide + glimepiride + glipizide + glyburide + pioglitazone + rosiglitazone + insulin + diabetesMed"
lm.input <- "readmitted ~ race + gender + time_in_hospital + num_lab_procedures + num_medications + number_emergency + number_inpatient + number_diagnoses + max_glu_serum + A1Cresult + metformin + glimepiride + glipizide + glyburide + pioglitazone + rosiglitazone + insulin + diabetesMed + disch_disp_modified + adm_src_mod + adm_typ_mod + age_mod"

# TODO -- if time try forward stepwise, backward, etc. using these variables then use AIC, BIC, Cp to come up with best (other) w/ optimal # variables
fit.min.lm <- glm(lm.input, data=data1, family = "binomial")
lm.output <- coef(fit.min.lm) # output lm estimates
summary(fit.min.lm) 
Anova(fit.min.lm)
```

After some initial modeling, I opted for a more robust and methodically sound model used LASSO regularization. This method has the advantage of model building that adds constraints (a penalty) to the variable coefficients, giving us sparse model selection. This is good given the amount of features (many with multiple levels) that we have. The tuning parameter for this penalty function - $\lambda$ - was chosen via cross validation, the value of which was chosen to minimize mean cross validation errors (`lambda.min`). This yielded a model with 22 variables (not double counting variable levels). Creating a model with these 22 variables and running the `Anova()` test revealed that not all variables were statistically significant. Thus, I began the process of (manual) backwards elimination by kicking out the variable with the largest P value (there is a large chance that the true value is zero (null hypothesis)).

```{r, include=FALSE}
#kick out race first -- highest p value at .85 (null hypoth can't be disproven)
fit.min.lm.1 <- update(fit.min.lm, .~. -race)
# Anova(fit.min.lm.1)

#kick out gender - .56
fit.min.lm.2 <- update(fit.min.lm.1, .~. -gender)
# Anova(fit.min.lm.2)

#kick out glyburide- 0.4748421 
fit.min.lm.3 <- update(fit.min.lm.2, .~. -glyburide)
# Anova(fit.min.lm.3)

#kick out num_lab_procedures - 0.3727800
fit.min.lm.4 <- update(fit.min.lm.3, .~. -num_lab_procedures)
# Anova(fit.min.lm.4)

#kick out adm_typ_mod - 0.3303114
fit.min.lm.5 <- update(fit.min.lm.4, .~. -adm_typ_mod)
# Anova(fit.min.lm.5)

#kick out pioglitazone - 0.3018096 
fit.min.lm.6 <- update(fit.min.lm.5, .~. -pioglitazone)
# Anova(fit.min.lm.6)

#kick out rosiglitazone- 0.244099 
fit.min.lm.7 <- update(fit.min.lm.6, .~. -rosiglitazone)
# Anova(fit.min.lm.7)

#kick out num_medications - 0.1380759
fit.min.lm.8 <- update(fit.min.lm.7, .~. -num_medications)
# Anova(fit.min.lm.8)

#kick out max_glu_serum - 0.1009794
fit.min.lm.9 <- update(fit.min.lm.8, .~. -max_glu_serum)
# Anova(fit.min.lm.9)

#kick out glimepiride -  0.0777974
fit.min.lm.10 <- update(fit.min.lm.9, .~. -glimepiride)
# Anova(fit.min.lm.10)

#kick out glimepiride -0.0153940 * 
# Glimepiride is a prescription drug. It comes as an oral tablet.
# Glimepiride is available as the brand-name drug Amaryl and as a generic drug. Generic drugs usually cost less. In some cases, they may not be available in every strength or form as the brand-name version.
# This drug may be used as part of a combination therapy. That means you need to take it with other drugs.
# Glimepiride is used to reduce high blood sugar levels in people with type 2 diabetes. It’s used in combination with a healthy diet and exercise.
# This medication may be used with insulin or other types of diabetes drugs to help control your high blood sugar.
```

After doing manual backwards elimination the model has 12 variables, all of which are statistically significant at the .05 level. Some of these remaining variables seem like they might exhibit colinearity. Although plotting them shows that the actuality is really not all that bad (see [appendix](#investigating-collinearity) ).

```{r, include=FALSE}
summary(fit.min.lm.10)
```

In looking at the summary of the model multiple levels of the `A1Cresult` are not significant. Also under further investigation this number seems likely to be highly correlated with insulin. This is a similar case with `metformin`. For those reasons I've decided to remove them. 

```{r, include=FALSE}
# The more glucose that enters the bloodstream, the higher the amount of glycated hemoglobin,” Dr. Dodell says. An A1C level below 5.7 percent is considered normal. An A1C between 5.7 and 6.4 percent signals prediabetes. Type 2 diabetes is diagnosed when the A1C is over 6.5 percent.

# fit.min.lm.11 <- update(fit.min.lm.10, .~. -glimepiride)
# Anova(fit.min.lm.11)

fit.min.lm.11 <- update(fit.min.lm.10, .~. -A1Cresult)
#Anova(fit.min.lm.11)

fit.min.lm.12 <- update(fit.min.lm.11, .~. -metformin)
# Anova(fit.min.lm.12)

# summary(fit.min.lm.12)
```

Lastly, for the sake of model robustness I am also choosing to remove `glipizide`, which is only significant at the .05 level according to the Anova test.

```{r, include=FALSE}
fit.min.lm.13 <- update(fit.min.lm.12, .~. -glipizide)
# Anova(fit.min.lm.13)
```

### Classifier 1 (from logistic regression)

The final model specified by preliminary model building using LASSO (least absolute shrinkage and selection operator): the resulting model obtained by LASSO regularization has 9 variables, 5 of which have multiple levels: 

```
Coefficients:
                  Estimate Std. Error z value Pr(>|z|)    
(Intercept)      -4.151458   0.584642  -7.101 1.24e-12 ***
age[10-20)        0.889354   0.605864   1.468 0.142128    
age[20-30)        1.424425   0.588665   2.420 0.015531 *  
age[30-40)        1.409278   0.585919   2.405 0.016162 *  
age[40-50)        1.332041   0.584615   2.278 0.022697 *  
age[50-60)        1.276257   0.584303   2.184 0.028945 *  
age[60-70)        1.437881   0.584173   2.461 0.013840 *  
age[70-80)        1.498978   0.584121   2.566 0.010282 *  
age[80-90)        1.512456   0.584315   2.588 0.009642 ** 
age[90-100)       1.436819   0.587038   2.448 0.014382 *  
time_in_hospital  0.022443   0.003758   5.971 2.35e-09 ***
num_procedures   -0.025263   0.006791  -3.720 0.000199 ***
num_medications   0.005140   0.001572   3.269 0.001079 ** 
number_emergency  0.032809   0.008436   3.889 0.000101 ***
number_inpatient  0.265180   0.006509  40.741  < 2e-16 ***
number_diagnoses  0.041705   0.006065   6.876 6.14e-12 ***
insulinNo        -0.168857   0.035189  -4.799 1.60e-06 ***
insulinSteady    -0.151091   0.033016  -4.576 4.73e-06 ***
insulinUp        -0.092093   0.039477  -2.333 0.019657 *  
diabetesMedYes    0.116898   0.031464   3.715 0.000203 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```

<!-- ##TODO redo this to reflect multiple logistic regression -->
<!-- $  Y = age + .02 * time\_in\_hospital - .03 * num\_procedures + .005 * num\_medications + .033 * number\_emergency + .27 * number\_inpatient + .04 * number\_diagnoses + insulin + diabetesMed - 4.151 $ -->

```{r, include=FALSE}
summary(fit.min.lm.13)
#XXXXXXXXXXXXX OLD XXXXXXXXX
# formula = readmitted ~ age + time_in_hospital + num_procedures + 
#     num_medications + number_emergency + number_inpatient + number_diagnoses + 
#     insulin + diabetesMed

# ******UPDATED*******
# readmitted ~ time_in_hospital + number_emergency + 
#     number_inpatient + number_diagnoses + insulin + diabetesMed + 
#     disch_disp_modified + adm_src_mod + age_mod
```

```{r, include=FALSE}
#nrow(data1) # 99493
pred.row <- data1[99493,] #take last observation for prediction
pred.row
fit13.predict <- predict(fit.min.lm.13, pred.row, type="response")
fit13.predict #0.07694511 
```

Doing a simple prediction with this model (which would be the goal) using the last observation in the dataset sets the prediction of this patient being readmitted in 30 days is 7.7%. 

##### Classifier and threshold 
<!-- Based on a quick and somewhat arbitrary guess, we estimate it costs twice as much to mislabel a readmission than it does to mislabel a non-readmission. Based on this risk ratio, propose a specific classification rule to minimize the cost. If you find any information that could provide a better cost estimate, please justify it in your write-up and use the better estimate in your answer. -->
Now that we have a logistic model to serve as a classifier we have to choose a probability threshold for what will be classified as a likely readmit <30 days, and what will not be classified as such. The goal will obviously be to minimize misclassification error (false positives & false negatives out of total observations). Due to the fact that we have estimated that mislabeling a readmission will incur a cost twice that of mislabeling a non-readmission we can use Bayes rule to take into account the weight of these potential losses. According to Bayes' rule we have:

$$P(Y=1 \vert x) > \frac{\frac{a_{0,1}}{a_{1,0}}}{1 + \frac{a_{0,1}}{a_{1,0}}}$$
Which given our cost estimation becomes: 

$$P(Y=1 \vert x) > \frac{\frac{1}{2}}{1 + \frac{1}{2}} = .33$$
$$logit > \log(\frac{0.33}{0.66})=-0.69$$

```{r, include=FALSE}
summary(fit.min.lm.13)
nrow(data1)
fit13.pred.bayes <- rep("0", 99493)
# fit13.pred.bayes
fit13.pred.bayes[fit.min.lm.13$fitted.values > 0.33] = "1"
MCE.bayes = (sum(2*(fit13.pred.bayes[data1$readmitted == "1"] != "1")) + sum(fit13.pred.bayes[data1$readmitted == "0"] != "0"))/length(data1$readmitted)
MCE.bayes #0.2224679
```

After using the information about false positives costing twice as much as false negatives to establish a threshold we see that the resulting mis-classification error is 22%, i.e. roughly 1 out of 5 patients would be mis-classified with this model and threshold.

##### Using testing data to assess classifier

```{r, include=FALSE}
library(pROC)
data.lastrow <- data1[-nrow(data1),] # last row out
dim(data1) #99493    29
# ~100,000, so 20% for testing would be ~20,000
N <- length(data1$readmitted)
set.seed(20)
index.train <- sample(N, 80000)
data.train <- data1[index.train,]
data.test <- data1[-index.train,]
# pairs(data.test, col=data.test$readmitted)
# dim(data.train) #80,000
# dim(data.test) #19,493
# input.testtrain = readmitted ~ age + time_in_hospital + num_procedures + num_medications + number_emergency + number_inpatient + number_diagnoses + insulin + diabetesMed
input.testtrain  <- "readmitted ~ time_in_hospital + number_emergency + number_inpatient + number_diagnoses + insulin + diabetesMed + disch_disp_modified + adm_src_mod + age_mod"
names(data.train)
fit.train <- glm(input.testtrain, data=data.train, family=binomial)
fit.fitted.test <- predict(fit.train, data.test, type="response")
# fit.test.roc <- roc(data.test$readmitted, fit.fitted.test, plot=T)
```

```{r, echo=FALSE}
fit.test.roc <- roc(data.test$readmitted, fit.fitted.test, plot=T)
```


While we now have a working model to predict patient readmission <30 days, for the sake of due diligence I will obtain another classifier using logistic regression, before obtaining a third using random forests.

<!---
=======================================================================================================================================================
=================================================== LASSO & MODEL SELECTION ===============================================================================
=======================================================================================================================================================
--->

#### Model building (classifier 2) using non-zero coefficients from LASSO with forward & backward stepwise selection

As a reminder the non-zero coefficients given by LASSO were:

```
readmitted ~ race + gender + time_in_hospital + num_lab_procedures + num_medications + number_emergency + number_inpatient + number_diagnoses + max_glu_serum + A1Cresult + metformin + glimepiride + glipizide + glyburide + pioglitazone + rosiglitazone + insulin + diabetesMed + disch_disp_modified + adm_src_mod + adm_typ_mod + age_mod
```

```{r, include=FALSE}
lm.input
```

I will perform forward and backward stepwise selection on these variables to get the smallest possible model with statistically significant coefficients. This is to say using minimum BIC as criterion I will choose the appropriate number of variables from each method, then compare outputs. 

#### Forward stepwise selection

```{r, include=FALSE}
#<<<<<<Forward stepwise selection>>>>>>>
library(leaps)
fit.fwd=regsubsets(readmitted ~ race + gender + time_in_hospital + num_lab_procedures + num_medications + number_emergency + number_inpatient + number_diagnoses + max_glu_serum + A1Cresult + metformin + glimepiride + glipizide + glyburide + pioglitazone + rosiglitazone + insulin + diabetesMed + disch_disp_modified + adm_src_mod + adm_typ_mod + age_mod,data=data1 ,nvmax =22, method ="forward")
# summary(fit.fwd)
fit.fwd.summary <-summary(fit.fwd)
# names(fit.fwd.summary)
# criteria plots
par(mfrow=c(2,2))
plot(fit.fwd.summary$rss,xlab="Number of Variables", ylab="RSS",type="l")
plot(fit.fwd.summary$adjr2,xlab="Number of Variables", ylab="Adjusted RSq",type="l")
# which.max(fit.fwd.summary$adjr2)
points(23,fit.fwd.summary$adjr2[23], col="red",cex=2,pch=20) # 23 variables
plot(fit.fwd.summary$cp,xlab="number of variables",ylab="Cp", type = 'l')
# which.min(fit.fwd.summary$cp)
points(which.min(fit.fwd.summary$cp),fit.fwd.summary$cp[which.min(fit.fwd.summary$cp)],col="red",cex=2,pch=20) # 21 variables
plot(fit.fwd.summary$bic,xlab="number of variables",ylab="BIC", type = 'l')
# which.min(fit.fwd.summary$bic)
points(which.min(fit.fwd.summary$bic),fit.fwd.summary$bic[which.min(fit.fwd.summary$bic)],col="red",cex=2,pch=20) # 10 variables
# choose to go with 10 variables --> choice that minimizes BIC
coef.fit.fwd <- coef(fit.fwd,10)
var.min <- rownames(as.matrix(coef.fit.fwd)) # output the names
# var.min
# print("============ 10 variable model using forwards stepwise selection==========")
# lm.input3 <- as.formula(paste("readmitted", "~", paste(var.min[-1], collapse = "+"))) # prepare for lm fomulae
# lm.input3 <- "readmitted ~ age + num_lab_procedures + number_inpatient + number_diagnoses + max_glu_serum + metformin + race"
lm.input3 <- "readmitted ~ num_lab_procedures + number_inpatient + number_diagnoses + max_glu_serum + metformin + disch_disp_modified + adm_src_mod + age_mod"
# coef.fit.fwd
# print("============BIC & Cp with 10 variables==========")
# fit.fwd.summary$bic[10] #-3237.99
# fit.fwd.summary$cp[10] #66.672
```

```{r, echo=FALSE}
par(mfrow=c(2,2))
plot(fit.fwd.summary$rss,xlab="Number of Variables", ylab="RSS",type="l")
plot(fit.fwd.summary$adjr2,xlab="Number of Variables", ylab="Adjusted RSq",type="l")
# which.max(fit.fwd.summary$adjr2)
points(23,fit.fwd.summary$adjr2[23], col="red",cex=2,pch=20) # 23 variables
plot(fit.fwd.summary$cp,xlab="number of variables",ylab="Cp", type = 'l')
# which.min(fit.fwd.summary$cp)
points(which.min(fit.fwd.summary$cp),fit.fwd.summary$cp[which.min(fit.fwd.summary$cp)],col="red",cex=2,pch=20) # 21 variables
plot(fit.fwd.summary$bic,xlab="number of variables",ylab="BIC", type = 'l')
# which.min(fit.fwd.summary$bic)
points(which.min(fit.fwd.summary$bic),fit.fwd.summary$bic[which.min(fit.fwd.summary$bic)],col="red",cex=2,pch=20) # 10 variables
```


#### Backward stepwise selection

```{r, include=FALSE}
#<<<<<<Backward stepwise selection>>>>>>>
library(leaps)
fit.bwd=regsubsets(readmitted ~ race + gender + time_in_hospital + num_lab_procedures + num_medications + number_emergency + number_inpatient + number_diagnoses + max_glu_serum + A1Cresult + metformin + glimepiride + glipizide + glyburide + pioglitazone + rosiglitazone + insulin + diabetesMed + disch_disp_modified + adm_src_mod + adm_typ_mod + age_mod, data=data1 ,nvmax =22,method ="backward")
# summary(fit.bwd)
fit.bwd.summary <-summary(fit.bwd)
# names(fit.bwd.summary)
# criteria plots
# par(mfrow=c(2,2))
# plot(fit.bwd.summary$rss,xlab="Number of Variables", ylab="RSS",type="l")
# plot(fit.bwd.summary$adjr2,xlab="Number of Variables", ylab="Adjusted RSq",type="l")
# which.max(fit.bwd.summary$adjr2)
# points(which.max(fit.bwd.summary$adjr2),fit.bwd.summary$adjr2[which.max(fit.bwd.summary$adjr2)], col="red",cex=2,pch=20) # 23 variables
# plot(fit.bwd.summary$cp,xlab="number of variables",ylab="Cp", type = 'l')
# which.min(fit.bwd.summary$cp)
# points(which.min(fit.bwd.summary$cp),fit.bwd.summary$cp[which.min(fit.bwd.summary$cp)],col="red",cex=2,pch=20) # 23 variables
# plot(fit.bwd.summary$bic,xlab="number of variables",ylab="BIC", type = 'l')
# which.min(fit.bwd.summary$bic)
# points(which.min(fit.bwd.summary$bic),fit.bwd.summary$bic[which.min(fit.bwd.summary$bic)],col="red",cex=2,pch=20) # 9 variables
# choose to go with 9 variables
coef.fit.bwd <- coef(fit.bwd,9)
var.min <- rownames(as.matrix(coef.fit.bwd)) # output the names -- SEVERAL WERE multi-level repeats
# var.min
# print("============ 9 variable model using backwards stepwise==========")
# lm.input4 <- as.formula(paste("violentcrimes.perpop", "~", paste(var.min[-1], collapse = "+"))) 

## due to mult levels for same variable there are actually only 7 vars
# lm.input4 <- "readmitted ~ age + num_lab_procedures + number_inpatient + number_diagnoses + max_glu_serum + metformin + insulin + race"
lm.input4 <- "readmitted ~ number_inpatient + number_diagnoses + max_glu_serum + metformin + disch_disp_modified + adm_src_mod + age_mod" 
# coef.fit.bwd
# print("============BIC with 9 variables==========")
# fit.bwd.summary$bic[9] #-3233.282
# fit.bwd.summary$cp[9] #80.89599
```

```{r, echo=FALSE}
par(mfrow=c(2,2))
plot(fit.bwd.summary$rss,xlab="Number of Variables", ylab="RSS",type="l")
plot(fit.bwd.summary$adjr2,xlab="Number of Variables", ylab="Adjusted RSq",type="l")
# which.max(fit.bwd.summary$adjr2)
points(which.max(fit.bwd.summary$adjr2),fit.bwd.summary$adjr2[which.max(fit.bwd.summary$adjr2)], col="red",cex=2,pch=20) # 23 variables
plot(fit.bwd.summary$cp,xlab="number of variables",ylab="Cp", type = 'l')
# which.min(fit.bwd.summary$cp)
points(which.min(fit.bwd.summary$cp),fit.bwd.summary$cp[which.min(fit.bwd.summary$cp)],col="red",cex=2,pch=20) # 23 variables
plot(fit.bwd.summary$bic,xlab="number of variables",ylab="BIC", type = 'l')
# which.min(fit.bwd.summary$bic)
points(which.min(fit.bwd.summary$bic),fit.bwd.summary$bic[which.min(fit.bwd.summary$bic)],col="red",cex=2,pch=20) # 9 variables
```


<!--- forward, backward & manual backward elimination --->
```{r, include=FALSE}
# OUTPUTS from forward & backward stepwise selection:
# <<< forward >>>
# XXXXXX lm.input3 <- "readmitted ~ age + num_lab_procedures + number_inpatient + number_diagnoses + max_glu_serum + metformin + race" XXXXXXXX
lm.input3 <- "readmitted ~ num_lab_procedures + number_inpatient + number_diagnoses + max_glu_serum + metformin + disch_disp_modified + adm_src_mod + age_mod"

# <<< backward >>>
# XXXXXX lm.input4 <- "readmitted ~ age + num_lab_procedures + number_inpatient + number_diagnoses + max_glu_serum + metformin + race + insulin" XXXXXXZ
lm.input4 <- "readmitted ~ number_inpatient + number_diagnoses + max_glu_serum + metformin + disch_disp_modified + adm_src_mod + age_mod"

# for comparison -- from manual backwards elimination
# XXXXXXX readmitted ~ age + number_inpatient + number_diagnoses + insulin + time_in_hospital + num_procedures + num_medications + number_emergency + diabetesMed
#             readmitted ~ time_in_hospital + number_emergency + number_inpatient + number_diagnoses + insulin + diabetesMed + disch_disp_modified + adm_src_mod + age_mod
```

Putting the variables chosen from forward & backward selection we see that the results are almost identical. The only difference is that forward selection pointed to the importance of one more variable than forward selection, `num_lab_procedures`. For this reason I've chosen to take the variables from forward selection as the inputs for a logistic regession model. Note that the BIC value is lower with this model as well. 

```{r, include=FALSE}
# LASSO --> forward/backward selection --> best vars are from FORWARD (most comprehensive -- 1 var more than backward, BIC is also lower) --> logistic regression
# fit.log.2 <- glm(lm.input3, data1, family= binomial)
# save(fit.log.2, file="/Users/josephhaymaker/Desktop/STAT571_Modern-data-mining/HW4/fit_log_2.RData")
load("/Users/josephhaymaker/Desktop/STAT571_Modern-data-mining/HW4/fit_log_2.RData")
# summary(fit.log.2)
Anova(fit.log.2)

# num_lab_procedures - 0.0252814
# fit.log.2.1 <- update(fit.log.2, .~. - num_lab_procedures)
# save(fit.log.2.1, file="/Users/josephhaymaker/Desktop/STAT571_Modern-data-mining/HW4/fit_log_2_1.RData")
load("/Users/josephhaymaker/Desktop/STAT571_Modern-data-mining/HW4/fit_log_2_1.RData")
Anova(fit.log.2.1)

# removing max_glu_serum - p = 0.0447352
# fit.log.2.2 <- update(fit.log.2.1, .~. - max_glu_serum)
# save(fit.log.2.2, file="/Users/josephhaymaker/Desktop/STAT571_Modern-data-mining/HW4/fit_log_2_2.RData")
load("/Users/josephhaymaker/Desktop/STAT571_Modern-data-mining/HW4/fit_log_2_2.RData")
Anova(fit.log.2.2)
summary(fit.log.2.2)
```

All variables are significant at the .05 level according to the Anova test in this model, but for robustness the two with the highest P values are being removed. After removing these two variables (`num_lab_procedures` and `max_glu_serum`) all variables are significant at the .01 level according to the Anova test. The misclassification error is 22%, similar to that of the first classifier.

<!--- just testing to see what the backwards vars have to say
```{r}
fit.log.3 <- glm(lm.input4, data1, family= binomial)
Anova(fit.log.3)

fit.log.3.1 <- update(fit.log.3, .~. - max_glu_serum)
Anova(fit.log.3.1)
summary(fit.log.3.1)
```
--->

```{r, include=FALSE}
# treating as classifier and assessing fit
nrow(data1) #99493
fit.log2.pred <- rep("0", 99493)
# fit.log2.pred
fit.log2.pred[fit.log.2.2$fitted.values > 0.33] = "1"
MCE.log2 = (sum(2*(fit.log2.pred[data1$readmitted == "1"] != "1")) + sum(fit.log2.pred[data1$readmitted == "0"] != "0"))/length(data1$readmitted)
MCE.log2 #0.2225383
```

### Classifier 2 (using logistic regression)

This formula for this classifier is:

```
readmitted ~ number_inpatient + number_diagnoses + 
    metformin + disch_disp_modified + adm_src_mod + age_mod
```

and the values of the coefficients are:

```
Coefficients:
                                                                Estimate Std. Error z value Pr(>|z|)    
(Intercept)                                                    -3.128193   0.206342 -15.160  < 2e-16 ***
number_inpatient                                                0.275203   0.006243  44.079  < 2e-16 ***
number_diagnoses                                                0.042862   0.005929   7.229 4.86e-13 ***
metforminNo                                                    -0.174865   0.130759  -1.337 0.181123    
metforminSteady                                                -0.257777   0.132659  -1.943 0.051998 .  
metforminUp                                                    -0.400679   0.172565  -2.322 0.020239 *  
disch_disp_modifiedDischarged to home with Home Health Service  0.231017   0.031307   7.379 1.59e-13 ***
disch_disp_modifiedDischarged/Transferred to SNF                0.423541   0.030362  13.950  < 2e-16 ***
disch_disp_modifiedOther                                        0.434021   0.028540  15.207  < 2e-16 ***
adm_src_modOther                                               -0.098864   0.041641  -2.374 0.017588 *  
adm_src_modPhysician Referral                                   0.008675   0.023791   0.365 0.715373    
adm_src_modTransfer from Home Health                           -0.116594   0.042870  -2.720 0.006534 ** 
age_mod20-59                                                    0.499108   0.160295   3.114 0.001848 ** 
age_mod60-79                                                    0.583015   0.160324   3.636 0.000276 ***
age_mod80+                                                      0.524009   0.161610   3.242 0.001185 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```

##### Using testing data to assess classifier

```{r, include=FALSE}
library(pROC)
data.lastrow <- data1[-nrow(data1),] # last row out
dim(data1) #99493    31
# ~100,000, so 20% for testing would be ~20,000
N <- length(data1$readmitted)
set.seed(20)
index.train.log2 <- sample(N, 80000)
data.train.log2 <- data1[index.train.log2,]
data.test.log2 <- data1[-index.train.log2,]
# pairs(data.test, col=data.test$readmitted)
# dim(data.train) #80,000
# dim(data.test) #19,493
input.testtrain.log2 = "readmitted ~ number_inpatient + number_diagnoses + max_glu_serum + metformin + disch_disp_modified + adm_src_mod + age_mod"
fit.train.log2 <- glm(input.testtrain.log2, data=data.train.log2, family=binomial)
fit.fitted.test.log2 <- predict(fit.train.log2, data.test.log2, type="response")

# fit.test.roc.log2 <- roc(data.test.log2$readmitted, fit.fitted.test.log2, plot=T)
```

Splitting the observations into testing and training data we can then plot the ROC curve:

```{r, echo=FALSE}
fit.test.roc.log2 <- roc(data.test.log2$readmitted, fit.fitted.test.log2, plot=T)
```


```{r, include=FALSE}
#nrow(data1) # 99493
pred.row <- data1[99493,] #take last observation for prediction
pred.row
fit.log2.predict <- predict(fit.train.log2, pred.row, type="response")
fit.log2.predict #0.08841787
```

Lastly, this classifier takes the last row observation and predicts it's `readmitted` value to be 8.84%, similar to the 7.7% predicted by the first classifier. 

<!---
### Comparing initial 2 classifiers
Actually all 3 classifiers
```{r, eval=FALSE, include=FALSE}
plot(1-fit.test.roc$specificities, fit.test.roc$sensitivities, col="red", pch=20,
     xlab = paste("AUC(fit.fitted.test) =",
                  round(auc(fit.test.roc),2),
                  " AUC(pred.rf2) =",
                  round(auc(fit.rf.train.roc),2),
                  " AUC(fit.test.roc.log2) =",
                  round(auc(fit.test.roc.log2),2)),
    ylab = "Sensitivities")

points(1-fit.rf.train.roc$specificities, fit.rf.train.roc$sensitivities, col="blue", pch=20)
points(1-fit.test.roc.log2$specificities, fit.test.roc.log2$sensitivities, col="yellow", pch=20)
# legend("topleft", legend=c("fit.fitted.test using logistic regression", "pred.rf2 using random forest"),
#        lty=c(1,1), lwd=c(2,2), col=c("red","blue"))
# title("Comparison of logistic and random forest model for prediction patient readmission")
title ("Sensitivity (true positive rate)  vs. false positive rate (1-specificity): logistic and random forest models")
```
--->
<!---
=======================================================================================================================================================
=================================================== LASSO & MODEL SELECTION -- END ====================================================================
=======================================================================================================================================================
--->

### Second model approach: random forests for classification

The second approach to model building will use random forests to develop the third and final classifier.

```{r, include=FALSE}
set.seed(1)
summary(data1)
levels(data1$diag_3)
names(data1)
fit.rf <- randomForest(readmitted~. -diag1_mod -diag2_mod -diag3_mod, data1, mtry=8, ntree=5) #removing vars with too many levels
# names(fit.rf)
fit.rf$votes[0:20,] #prob of 0s and 1s using oobs
fit.rf$predicted[0:20]
fit.rf$err.rate #mis-classification errors of oob's 0/1
# Out-of-bag (OOB) error, also called out-of-bag estimate, is a method of measuring the prediction error of random forests, boosted decision trees, and other machine learning models utilizing bootstrap aggregating to sub-sample data samples used for training. OOB is the mean prediction error on each training sample xᵢ, using only the trees that did not have xᵢ in their bootstrap sample.
predict(fit.rf, data1)[1:20]
# data.frame(fit.rf$votes[1:20,], fit.rf$predicted[1:20], predict(fit.rf, data1)[1:20])
plot(fit.rf)
```

```{r, echo=FALSE}
plot(fit.rf)
```

Note that the variables `diag1_mod`, `diag2_mod`, and `diag3_mod` were all removed from modeling to begin with due to the fact that they all have a large number of levels. We can see (above) using a forest with just a few number of trees results in mean prediction error designated by the black line is around 20% (less than desirable). Expanding the number of trees we get:

```{r, echo=FALSE}
# fit.rf.full <- randomForest(readmitted~. -diag1_mod -diag2_mod -diag3_mod, data1, mtry=5, ntree=500)
# save(fit.rf.full, file="/Users/josephhaymaker/Desktop/STAT571_Modern-data-mining/HW4/fit.rf.full.RData")
load("/Users/josephhaymaker/Desktop/STAT571_Modern-data-mining/HW4/fit.rf.full.RData")

plot(fit.rf.full)
fit.rf.pred <- predict(fit.rf.full, type="prob")
fit.rf.pred.y <- predict(fit.rf.full, type = "response")
# mean(data1$readmitted != fit.rf.pred.y) #training misclassification error is 0.1123295
```

As the graph shows when we expand the number of trees in the forest to 500 we get a misclassification error now around 11%. To assure overfitting in this model we now split the dataset into training (80%) and testing data (20% of original data). Plotting the model using the training data is almost identical to the full dataset findings (which is good).

```{r, include=FALSE}
# using testing and training data 
set.seed(10)
n <- nrow(data1)
n1 <- (4/5)*n
train.index.rf <- sample(n, n1, replace = FALSE)
length(train.index.rf) #79594

#random forest training data -- 80% of original data
data.train.rf <- data1[train.index.rf,]

#random forest testing data -- remaing 20% of original data
data.test.rf <- data1[-train.index.rf,]

#fitting a model with training data
# fit.rf.train <- randomForest(readmitted~. -diag1_mod -diag2_mod -diag3_mod, data.train.rf)
# save(fit.rf.train, file="/Users/josephhaymaker/Desktop/STAT571_Modern-data-mining/HW4/fit.rf.train.RData")
load("/Users/josephhaymaker/Desktop/STAT571_Modern-data-mining/HW4/fit.rf.train.RData")
```

```{r, eval=FALSE, include=FALSE}
plot(fit.rf.train)
```

```{r, include=FALSE}
predict.rf.y2 <- predict(fit.rf.train, newdata = data.test.rf)
predict.rf2 <- predict(fit.rf.train, newdata = data.test.rf, type="prob")
mean(data.test.rf$readmitted != predict.rf.y2) #0.1118649 /  0.1108096
fit.rf.train.roc <- roc(data.test.rf$readmitted, predict.rf2[,2], plot = TRUE)
```

We now have 3 classifiers (two from logistic regression and now one using random forests), and thus have 3 ROC curves to compare:

```{r, echo=FALSE, eval=FALSE}
fit.train
# fit.test.roc #roc curve LASSO, logistic regression, manual backwards elimination
# summary(fit.rf.train) #random forest w/ B= 500 trees, mtry = 5
fit.rf.train
# fit.rf.train.roc #roc curve random forest

plot(1-fit.test.roc$specificities, fit.test.roc$sensitivities, col="red", pch=20,
     xlab = paste("AUC(fit.fitted.test) =",
                  round(auc(fit.test.roc),2),
                  " AUC(pred.rf2) =",
                  round(auc(fit.rf.train.roc),2) ),
    ylab = "Sensitivities")

points(1-fit.rf.train.roc$specificities, fit.rf.train.roc$sensitivities, col="blue", pch=20)
legend("topleft", legend=c("fit.fitted.test using logistic regression", "pred.rf2 using random forest"),
       lty=c(1,1), lwd=c(2,2), col=c("red","blue"))
# title("Comparison of logistic and random forest model for prediction patient readmission")
title ("Sensitivity (true positive rate)  vs. false positive rate (1-specificity): logistic and random forest models")

#true positive rate (sensitivity) against the false positive rate (1-specificity)
```

```{r, echo=FALSE}
plot(1-fit.test.roc$specificities, fit.test.roc$sensitivities, col="red", pch=20,
     xlab = paste("AUC(fit.fitted.test) =",
                  round(auc(fit.test.roc),2),
                  " AUC(pred.rf2) =",
                  round(auc(fit.rf.train.roc),2),
                  " AUC(fit.test.roc.log2) =",
                  round(auc(fit.test.roc.log2),2)),
    ylab = "Sensitivities")

points(1-fit.rf.train.roc$specificities, fit.rf.train.roc$sensitivities, col="blue", pch=20)
points(1-fit.test.roc.log2$specificities, fit.test.roc.log2$sensitivities, col="yellow", pch=20)
legend("topleft", legend=c("fit.fitted.test using logistic regression", "pred.rf2 using random forest", "fit.test.roc.log2 using logistic regression"),
       lty=c(1,1), lwd=c(2,2), col=c("red","blue", "yellow"))
# title("Comparison of logistic and random forest model for prediction patient readmission")
title ("Sensitivity (true positive rate)  vs. false positive rate (1-specificity): logistic and random forest models")
```

We can see that when comparing the three classifiers that in terms of area under the curve alone the random forest model is worse than the logistic regression models by only .02. More importantly, the logistic regression models outperform the random forest model at practically every level. Since the two logistic models are almost identical, I'd give preference to the original classifier due to the fact that it maximizes the true positive rate slightly more than the second classifier at several points. 

### Conclusion

This study had as its goal to identify patient characteristics that affect readmission < 30 days in order to better serve patients and minimize healthcare costs. To this end, I first examined relevant variables before performing LASSO regularization with cross validation to decrease the number of variables under consideration. I then obtained 2 classifiers from logistic regression -- one via manual backwards elimination, and another as a result of the comparision of backwards and forward selection. The threshold used for classification was chosen using Baye's Rule, knowing that a false positive costs twice as much as a false negative. I then created one last classifier using random forests. All of these models were checked using testing and training data. I then plotted all three classifiers' ROC curves and compared them before concluding that the first was the best in terms of maximizing the true positive rate. The implications of this model are extensive. For the sake of clarity I have summarized them in the following table:

|Description|Variable name|Categories/levels (if applicable)|Effect on readmission <30 days|
|-----------|-------------|---------------------------------|------------------------------|
|Admission source/patient referral|`adm_src_mod`|emergency room, other, physician, or transfer from home health|compared to emergency room referrals both 'other' and home health transfers decrease readmission, while physician referrals only slightly increase it|
|Patient age|`age_mod`|0-19 yrs, 20-59 yrs, 60-79 yrs, & 80+ yrs|compared to 0-19 year old patients all other age brackets increase probability, with the 60-79 bracket assiated with the greatest increase|
|Diabetes medication prescribed (y/n)|`diabetesMed`|no or yes|a medication prescription is associated with an increase in readmission|
|Discharge disposition|`disch_disp_modified`|discharged to home, discharged to home w/ home health services, discharged/transferred to SNF (skilled nursing facilities), & other|compared to home discharge all other categories increase readmission likelihood|
|Insulin dose change|`insulin`|down, no, steady, or up|compared to a drop in dosage all other categories correspond to a decrease in readmission|
|Total diagnosis for patient|`number_diagnoses`|1-16|increases in diagnoses correspond with increase in readmission|
|Number of emergency visits by the patient in the year prior to the current encounter|`number_emergency`|0-76|increases in emergency visits correspond with increase in readmission|
|Number of inpatient visits by the patient in the year prior to the current encounter|`number_inpatient`|0-21|increases in inpatient visits correspond with increase in readmission|
|Patient’s length of stay in the hospital (in days)|`time_in_hospital`|1-14|increases in length of stay correspond with increase in readmission|

Knowing this information hospitals can now predict the likelihood that a given patient will be readmitted in less than thirty days, and adjust their intervention accordingly.

__________________________________________________________

# Appendix

### Variable names and detail explanations
id: variables

Description of variables

The dataset used covers ~50 different variables to describe every hospital diabetes admission. In this section we give an overview and brief description of the variables in this dataset.

**a) Patient identifiers:** 

a. `encounter_id`: unique identifier for each admission 
b. `patient_nbr`: unique identifier for each patient 

**b) Patient Demographics:** 

`race`, `age`, `gender`, `weight` cover the basic demographic information associated with each patient. `Payer_code` is an additional variable that identifies which health insurance (Medicare /Medicaid / Commercial) the patient holds.

**c) Admission and discharge details:** 

a.	`admission_source_id` and `admission_type_id` identify who referred the patient to the hospital (e.g. physician vs. emergency dept.) and what type of admission this was (Emergency vs. Elective vs. Urgent). 
b.	`discharge_disposition_id` indicates where the patient was discharged to after treatment.

**d) Patient Medical History:**

a.	`num_outpatient`: number of outpatient visits by the patient in the year prior to the current encounter
b.	`num_inpatient`: number of inpatient visits by the patient in the year prior to the current encounter
c.	`num_emergency`: number of emergency visits by the patient in the year prior to the current encounter

**e)	Patient admission details:**

a.	`medical_specialty`: the specialty of the physician admitting the patient
b.	`diag_1`, `diag_2`, `diag_3`: ICD9 codes for the primary, secondary and tertiary diagnoses of the patient.  ICD9 are the universal codes that all physicians use to record diagnoses. There are various easy to use tools to lookup what individual codes mean (Wikipedia is pretty decent on its own)
c.	`time_in_hospital`: the patient’s length of stay in the hospital (in days)
d.	`number_diagnoses`: Total no. of diagnosis entered for the patient
e.	`num_lab_procedures`: No. of lab procedures performed in the current encounter
f.	`num_procedures`: No. of non-lab procedures performed in the current encounter
g.	`num_medications`: No. of distinct medications prescribed in the current encounter

**f)	Clinical Results:**

a.	`max_glu_serum`: indicates results of the glucose serum test
b.	`A1Cresult`: indicates results of the A1c test

**g)	Medication Details:**

a.	`diabetesMed`: indicates if any diabetes medication was prescribed 
b.	`change`: indicates if there was a change in diabetes medication
c.	`24 medication variables`: indicate whether the dosage of the medicines was changed in any manner during the encounter

**h)	Readmission indicator:** 

Indicates whether a patient was readmitted after a particular admission. There are 3 levels for this variable: "NO" = no readmission, "< 30" = readmission within 30 days and "> 30" = readmission after more than 30 days. The 30 day distinction is of practical importance to hospitals because federal regulations penalize hospitals for an excessive proportion of such readmissions.

### Further EDA

##### Age

```{r, echo=FALSE}
# summary(data1$age_mod) #scatterplot
# names(data1)
#<<< SCATTERPLOT WITH LS LINE ADDED >>>>>
# lm.age <- lm(readmitted~age_mod, data = data1)
plot(data1$age_mod, data1$readmitted,
     pch = 16,
     xlab = "Patient age",
     ylab = "Readmission category",
     main = "Patient age vs. readmission category")
# abline(lm.age, col="red", lwd=4)
# abline(h=mean(county_data$dem12_frac), lwd=5, col="blue")
```

It appears that the categories with the largest number of readmits are 60-79 and 80+, which are almost identical. As expected these percentages fall as age category does, though not dramatically. 

##### Change (in diabetes medication)

```{r, echo=FALSE}
# summary(data1$change) #boxplot - change in diabetes medication
#    Ch    No 
# 47011 54755

# <30 readmit patients
readmit_less30.change <- filter(data1, readmitted == "<30")
# dim(readmit_less30.change) # 11357 total observations
# dim(filter(readmit_less30.change, change == "Ch")) #5558 patients with a change of med readmitted <30 days, 48.9% of all patients readmitted <30, 11.8% of all patients with a change in meds
# dim(filter(readmit_less30.change, change == "No")) #5799 patients with NO change in meds readmitted <30 days, 51.1% of all patients readmitted <30, 10.6% of all patients with NO change in meds

#>30 readmit patients
readmit_more30.change<- filter(data1, readmitted == ">30")
# dim(readmit_more30.change) #35545 observations
# dim(filter(readmit_more30.change, change == "Ch")) #17272
perc.readmit_more30.ch <- 17272/35545
# perc.readmit_more30.ch #0.4859193
perc.all.ch <- 17272/47011
# perc.all.ch #0.3674034
# dim(filter(readmit_more30.change, change == "No"))#18273
perc.readmit_more30.no <- 18273/35545
# perc.readmit_more30.no #0.5140807
perc.all.no <- 18273/54755
# perc.all.no #0.3337229


# pie charts 
par(mfrow = c(2, 1))
slices.change <- c(5558, 5799) 
lbls.change <- c("change in medication", "no change in medication")
pct.change <- round(slices.change/sum(slices.change)*100)
lbls.change <- paste(lbls.change, "-(",pct.change, ")") # add percents to labels 
lbls.change <- paste(lbls.change,"%",sep="") # ad % to labels 
pie(slices.change,labels = lbls.change, col=rainbow(length(lbls.change)),
  	main="Pie Chart of change in diabetes medication status for patients readmitted <30 days")
slices.nochange <- c(17272, 18273) 
lbls.nochange <- c("change in medication", "no change in medication")
pct.nochange <- round(slices.nochange/sum(slices.nochange)*100)
lbls.nochange <- paste(lbls.nochange, "-(",pct.nochange, ")") # add percents to labels 
lbls.nochange <- paste(lbls.nochange,"%",sep="") # ad % to labels 
pie(slices.nochange,labels = lbls.nochange, col=rainbow(length(lbls.nochange)),
  	main="Pie Chart of change in diabetes medication status for patients readmitted >30 days")
```

These figures show that roughly half of both types of readmits experienced a change in medication, while the other half did not. This appears to have little bearing on the outcome.

### Investigating collinearity

Some of the features provided seem like they might be highly correlated with one another. In particular, diagnoses & health indicator information, as well as hospital visit information.

Before initial modeling: 

```{r, echo=FALSE}
# hospital visit info
data1 %>%
  select_if(is.numeric) %>%
  select(time_in_hospital, num_lab_procedures, num_procedures, num_medications, number_outpatient, number_emergency, number_inpatient, number_diagnoses) %>%
  pairs(col=data1$readmitted) # base pair-wise scatter plots
```

```{r, eval=FALSE, include=FALSE}
# names(data1)
# summary(data1)
# data1 %>%
#   select(diag_1, diag_2, diag_3) %>%
#   pairs() # base pair-wise scatter plots
```

After LASSO:

```{r, echo=FALSE}
data1 %>% select(time_in_hospital, num_procedures, num_medications, number_emergency, number_inpatient, number_diagnoses) %>% cor()
# <<<<< CORRELATION OF VARIABLES >>>>>>>>>
data1 %>% 
  select_if(is.numeric) %>%
  select(time_in_hospital, num_procedures, num_medications, number_emergency, number_inpatient, number_diagnoses) %>%
  ggpairs()
```

<!---
To summarize, here are some salient differences between Lasso, Ridge and Elastic-net:

+ <<Lasso>> does a sparse selection, while Ridge does not.
+ When you have highly-correlated variables, <<Ridge regression>> shrinks the two coefficients towards one another. Lasso is somewhat indifferent and generally picks one over the other. Depending on the context, one does not know which variable gets picked. Elastic-net is a compromise between the two that attempts to shrink and do a sparse selection simultaneously.
+ <<Ridge estimators>> are indifferent to multiplicative scaling of the data. That is, if both X and Y variables are multiplied by constants, the coefficients of the fit do not change, for a given λλ parameter. However, for Lasso, the fit is not independent of the scaling. In fact, the λλ parameter must be scaled up by the multiplier to get the same result. It is more complex for elastic net.
+ Ridge penalizes the largest ββ's more than it penalizes the smaller ones (as they are squared in the penalty term). Lasso penalizes them more uniformly. This may or may not be important. In a forecasting problem with a powerful predictor, the predictor's effectiveness is shrunk by the Ridge as compared to the Lasso.

--->

<!--- BOOSTING
```{r, eval=FALSE}
set.seed(30)
data.boosting <- data1[sample(nrow(data1)),]
#summary(data.boosting)
# ntree <- 200
boost.in <-  readmitted~. -diag1_mod -diag2_mod -diag3_mod
# fit.boost <- gbm(boost.in, data = data.boosting, distribution = "gaussian", n.trees = ntree, interaction.depth = 2, train.fraction = .7)
# names(fit.boost)
# fit.boost$fit
# fit.boost$train.error
# fit.boost$valid.error
# yhat <- predict(fit.boost, newdata = data.boosting, n.trees = ntree)
# yhat
# gbm.perf(fit.boost, plot.it = TRUE, method = "test")

#investigate test/training error
ntree <- 10000
fit.boost <- gbm(boost.in, data = data.boosting, distribution = "gaussian", n.trees = ntree, interaction.depth = 2, train.fraction = .7)
gbm.perf(fit.boost, method="test")
min(fit.boost$valid.error)
min(fit.boost$train.error)
# length(fit.boost$train.error) #10000
# fit.boost[which.min(df$Amount),]
# class(fit.boost)
# which(fit.boost$train.error == 0.09526856)
```
Originally ran boosting with 2000 trees but a plot seemed to suggest that more trees would result in 
```{r, eval=FALSE}
# using testing and training data
nrow(data.boosting) #99493
n.t <- floor(.7*99493)
data.train.boost <- data.boosting[1:n.t,]
data.test.boost <- data.boosting[-(1:n.t),]
B <- gbm.perf(fit.boost, method="test") #optimal number of trees
B #10000
yhat <- predict(fit.boost,newdata = data.test.boost, n.trees = B)
length(yhat) #29848
yhat
length(data.test.boost$readmitted)
#fit.boost.test <- (mean(yhat != data.test.boost$readmitted))^2 #testing error for boosting
sum(yhat != data.test.boost$readmitted) #all different -> reject
```
--->

<!---
##### Modeling a single tree for interpretation purposes
```{r, eval=FALSE}
# summary(data1)
# fit.1tree <- tree(readmitted~. -diag_1 -diag_2 -diag_3, data1,
#                   control=tree.control(nrow(data1), mindev = 0.005),
#                   split="deviance")
# plot(fit.1tree)
# text(fit.1tree, pretty=TRUE)
data2 <- na.omit(data1)
# summary(data2)
# str(data2)
# fit.1tree <- rpart(readmitted~. -diag_1 -diag_2 -diag_3, data2, minsplit=1, cp=9e-3)
fit.1tree <- rpart(input.testtrain, data2, minsplit=5, cp=9e-3)
# plot(as.party(fit.1tree), main="Final tree with Rpart")
plot(fit.1tree)
fit.1tree
```
--->

<!---
##### Trying model building with bestglm() function
```{r, eval=FALSE}
library(bestglm)
best.input <- readmitted ~ age + time_in_hospital + num_procedures + num_medications + number_emergency + number_inpatient + number_diagnoses + insulin + diabetesMed
Xy <- model.matrix(best.input, data1)
Xy <- data.frame(Xy, data1$readmitted)
# str(Xy)
fit.best <- bestglm(Xy, family = binomial, method = "exhaustive", intercept = FALSE, IC = "AIC", nvmax = 10, data = data1)
```
never again
--->